{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from  langchain_classic.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_classic.chains import GraphQAChain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct',api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x11ce6e900>, async_client=<openai.resources.completions.AsyncCompletions object at 0x11ce6eba0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphrag():\n",
    "        with open('hybrid_rag.txt', 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        documents = [Document(page_content=content)]\n",
    "        llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "        graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "        graph = NetworkxEntityGraph()\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        for node in graph_documents[0].nodes:\n",
    "            graph.add_node(node.id)\n",
    "\n",
    "        # Add edges to the graph\n",
    "        for edge in graph_documents[0].relationships:\n",
    "            graph._graph.add_edge(\n",
    "                    edge.source.id,\n",
    "                    edge.target.id,\n",
    "                    relation=edge.type,\n",
    "                )\n",
    "\n",
    "            graph._graph.add_edge(\n",
    "                    edge.target.id,\n",
    "                    edge.source.id,\n",
    "                    relation=edge.type+\" by\",\n",
    "                )\n",
    "\n",
    "        chain = GraphQAChain.from_llm(\n",
    "            llm=llm, \n",
    "            graph=graph, \n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag():\n",
    "        #Document Loader\n",
    "        loader = TextLoader('hybrid_rag.txt')\n",
    "        data = loader.load()\n",
    "\n",
    "        #Document Transformer\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.split_documents(data)\n",
    "\n",
    "        #Vector DB\n",
    "        docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "        #Hyperparameters to know\n",
    "        retriever = docsearch.as_retriever(search_type='similarity_score_threshold',search_kwargs={\"k\": 7,\"score_threshold\":0.1})\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "        \n",
    "        return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_rag = rag()\n",
    "graph_rag = graphrag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushpande/Desktop/Agentic_ai/Graph_RAG/graphRAG/.venv/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:1043: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'hybrid_rag.txt'}, page_content='The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.'), 0.23384270235768823), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.'), 0.23380578171839272), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='A schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1\\n\\nFor the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.'), 0.23132493392134446), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='A schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1\\n\\nFor the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.'), 0.23122546735885874), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='KG based RAG [22], or GraphRAG, also begins with a query based on the user’s input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model’s internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the'), 0.06624002728272071), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='KG based RAG [22], or GraphRAG, also begins with a query based on the user’s input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model’s internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the'), 0.06598183568874294), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='generated outputs are accurate, contextually relevant, and grounded in verifiable information.'), -0.2650041091630697)]\n",
      "  self.vectorstore.similarity_search_with_relevance_scores(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard RAG:  The hybrid RAG technique integrates traditional vector-based retrieval with a KG-based retrieval system, combining broad similarity-based retrieval with structured, relationship-rich contextual data. It is utilized in a LLM to generate final responses and its implementation is detailed in section 4.4.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3m NONE\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Graph RAG:  Hybrid rag is a crossbreed between a ragdoll and another breed of cat. It can have a variety of coat colors and patterns, and typically has the laid-back and affectionate personality of a ragdoll.\n",
      "Hybrid: \n",
      "\n",
      "The hybrid RAG technique is a method of integrating traditional vector-based retrieval with a KG-based retrieval system. It combines broad similarity-based retrieval with structured, relationship-rich contextual data, and is used in a LLM to generate final responses. This technique is detailed in section 4.4. Additionally, a hybrid rag is a crossbreed between a ragdoll and another breed of cat, known for its variety of coat colors and patterns. It typically exhibits the laid-back and affectionate personality of a ragdoll.\n"
     ]
    }
   ],
   "source": [
    "def hybrid_rag(query,standard_rag,graph_rag):\n",
    "    result1 = standard_rag.run(query)\n",
    "    \n",
    "    print(\"Standard RAG:\",result1)\n",
    "    result2 = graph_rag.run(query)\n",
    "    \n",
    "    \n",
    "    print(\"Graph RAG:\",result2)\n",
    "    prompt = \"Generate a final answer using the two context given : Context 1: {} \\n Context 2: {} \\n Question: {}\".format(result1,result2,query)\n",
    "    #return llm(prompt)\n",
    "    return llm.generate([prompt]).generations[0][0].text\n",
    "\n",
    "query = \"Some characteristics of hybrid rag\"\n",
    "hybrid = hybrid_rag(query,standard_rag,graph_rag)\n",
    "print(\"Hybrid:\",hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushpande/Desktop/Agentic_ai/Graph_RAG/graphRAG/.venv/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:1043: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'source': 'hybrid_rag.txt'}, page_content='A schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1\\n\\nFor the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.'), 0.4383668596651692), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='A schematic diagram of the retrieval methodology of GraphRAG is given in Figure 2. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section 4.1\\n\\nFor the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.'), 0.43830675825919274), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='KG based RAG [22], or GraphRAG, also begins with a query based on the user’s input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model’s internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the'), 0.41226009017315723), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='KG based RAG [22], or GraphRAG, also begins with a query based on the user’s input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model’s internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company [8]. This integration helps ensure that the'), 0.41149360758613807), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.'), 0.37215601511588925), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section 4.4.'), 0.3721079677085868), (Document(metadata={'source': 'hybrid_rag.txt'}, page_content='generated outputs are accurate, contextually relevant, and grounded in verifiable information.'), -0.11503407392481657)]\n",
      "  self.vectorstore.similarity_search_with_relevance_scores(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard RAG:  The retrieval methodology of GraphRAG involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system. It utilizes a subgraph of relevant nodes and edges from the KG and combines it with the language model's internal knowledge. GraphRAG also leverages metadata information to selectively filter and retrieve only relevant document segments.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3m GraphRAG\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3mGraphRAG IS_DIFFERENT_TO VectorRAG\n",
      "GraphRAG HAS_INTEGRATION by language model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Graph RAG: \n",
      "\n",
      "I don't know.\n",
      "Hybrid: \n",
      "\n",
      "GraphRAG is a retrieval methodology that combines contextual information from both traditional vector-based retrieval and a KG-based retrieval system. It utilizes a subgraph of relevant nodes and edges from the KG and integrates it with the internal knowledge of a language model. Additionally, GraphRAG leverages metadata information to filter and retrieve only relevant document segments.\n"
     ]
    }
   ],
   "source": [
    "query = \"Some characteristics of GraphRAG\"\n",
    "hybrid = hybrid_rag(query,standard_rag,graph_rag)\n",
    "print(\"Hybrid:\",hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
